{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# torch model save and load\n",
    "\n",
    "- 문서 수정 : JPark @ KETI\n",
    "\n",
    "- 원문 : https://tutorials.pytorch.kr/recipes/recipes/save_load_across_devices.html\n",
    "\n",
    "- 원문의 제목은 \"PyTorch에서 다양한 장치 간 모델을 저장하고 불러오기\" 입니다.\n",
    "\n",
    "- 원문에서 제시한 방법을 m1 mac 환경에서 테스트하고, 문제가 발생하는 경우 이를 수정했습니다.\n",
    "\n",
    "- 원래의 파일명은 \"save load across devices\" 였으나 \"save load across local devices\"로 수정했습니다.\n",
    "\n",
    "- 원래의 파일명은 물리적으로 떨어진 장치들 사이에서도 모델을 읽고 쓸 수 있는 것처럼 느껴지지만, 본 문서에서는 하나의 컴퓨터 시스템에서 CPU와 GPU사이의 데이터 교환 방법을 다루고 있습니다.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## 아래와 같은 단계를 거칩니다.\n",
    "\n",
    "1. 데이터 활용에 필요한 모든 라이브러리 Import 하기\n",
    "\n",
    "2. 신경망을 구성하고 초기화하기\n",
    "\n",
    "3. GPU에서 저장하고 CPU에서 불러오기\n",
    "\n",
    "4. GPU에서 저장하고 GPU에서 불러오기\n",
    "\n",
    "5. CPU에서 저장하고 GPU에서 불러오기\n",
    "\n",
    "6. DataParallel 모델을 저장하고 불러오기\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "PyTorch에서 다양한 장치 간 모델을 저장하고 불러오기\n",
    "===================================================\n",
    "\n",
    "다양한 장치(device)에서 당신의 신경망 모델을 저장하거나 불러오고 싶은 \n",
    "경우가 생길 수 있습니다.\n",
    "\n",
    "개요\n",
    "------------\n",
    "\n",
    "PyTorch를 사용하여 장치 간의 모델을 저장하거나 불러오는 것은 비교적 간단합니다.\n",
    "이번 레시피에서는, CPU와 GPU에서 모델을 저장하고 불러오는 방법을 실험할 것입니다.\n",
    "\n",
    "설정\n",
    "-----\n",
    "\n",
    "이번 레시피에서 모든 코드 블록이 제대로 실행되게 하려면, \n",
    "우선 런타임(runtime) 설정을 \"GPU\"나 더 높게 지정해주어야 합니다. \n",
    "이후, 아래와 같이 ``torch``를 설치해야 PyTorch를 사용할 수 있습니다.\n",
    "\n",
    "::\n",
    "\n",
    "   pip install torch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "단계\n",
    "-----\n",
    "\n",
    "1. 데이터 활용에 필요한 모든 라이브러리 Import 하기\n",
    "2. 신경망을 구성하고 초기화하기\n",
    "3. GPU에서 저장하고 CPU에서 불러오기\n",
    "4. GPU에서 저장하고 GPU에서 불러오기\n",
    "5. CPU에서 저장하고 GPU에서 불러오기\n",
    "6. ``DataParallel`` 모델을 저장하고 불러오기\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 1. 데이터 활용에 필요한 모든 라이브러리 Import 하기\n",
    "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "\n",
    "이번 레시피에서 우리는 ``torch`` 및 하위 패키지인 ``torch.nn``와 \n",
    "``torch.optim``을 사용할 것입니다.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 신경망을 구성하고 초기화하기\n",
    "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "\n",
    "예로, 이미지 트레이닝을 위한 신경망을 생성해보겠습니다.\n",
    "자세한 내용은 신경망 정의 레시피를 참조하세요.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=400, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 16 * 5 * 5)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "net = Net()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. GPU에서 저장하고 CPU에서 불러오기\n",
    "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "\n",
    "GPU로 학습된 모델을 CPU에서 불러올 때는 ``torch.load()`` 함수의 \n",
    "``map_location`` 인자에 ``torch.device('cpu')``를 전달합니다.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 저장하고자 하는 경로를 지정합니다.\n",
    "PATH = \"model.pt\"\n",
    "\n",
    "# 저장하기\n",
    "torch.save(net.state_dict(), PATH)\n",
    "\n",
    "# 불러오기\n",
    "device = torch.device('cpu')\n",
    "model = Net()\n",
    "model.load_state_dict(torch.load(PATH, map_location=device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이 경우, Tensor의 저장된 내용은 ``map_location`` 인자를 통하여 CPU 장치에\n",
    "동적으로 재배치됩니다.\n",
    "\n",
    "# 4. GPU (CUDA) 에서 저장하고 GPU에서 불러오기\n",
    "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "\n",
    "GPU에서 학습하고 저장된 모델을 GPU에서 불러올 때는, 초기화된 모델에\n",
    "``model.to(torch.device('cuda'))``을 호출하여 CUDA에 최적화된 모델로 \n",
    "변환해주세요.\n",
    "\n",
    "그리고 모든 입력에 ``.to(torch.device('cuda'))`` 함수를 호출해야 \n",
    "모델에 데이터를 제공할 수 있습니다.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CUDA GPU 환경에서만 정상으로 동작합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[warning] cuda is not available\n"
     ]
    }
   ],
   "source": [
    "# 저장하기\n",
    "torch.save(net.state_dict(), PATH)\n",
    "\n",
    "\n",
    "# 불러오기\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    model = Net()\n",
    "    model.load_state_dict(torch.load(PATH))\n",
    "    model.to(device)\n",
    "else:\n",
    "    print('[warning] cuda is not available')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## 4-1. GPU (M1 Silicon GPU) 에서 저장하고 GPU에서 불러오기\n",
    "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "\n",
    "GPU에서 학습하고 저장된 모델을 GPU에서 불러올 때는, 초기화된 모델에\n",
    "``model.to(torch.device('mps'))``을 호출하여 CUDA에 최적화된 모델로 \n",
    "변환해주세요.\n",
    "\n",
    "그리고 모든 입력에 ``.to(torch.device('mps'))`` 함수를 호출해야 \n",
    "모델에 데이터를 제공할 수 있습니다.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok\n"
     ]
    }
   ],
   "source": [
    "# 저장하기\n",
    "torch.save(net.state_dict(), PATH)\n",
    "\n",
    "if torch.backends.mps.is_available():\n",
    "    # 불러오기\n",
    "    device = torch.device(\"mps\")\n",
    "    model = Net()\n",
    "    model.load_state_dict(torch.load(PATH))\n",
    "    model.to(device)\n",
    "    print(\"loading ok\")\n",
    "else:\n",
    "    print('[warning] mps is not available')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``my_tensor.to(device)``를 호출하면 GPU에 ``my_tensor``의 복사본이\n",
    "반환되며, 이는 ``my_tensor``를 덮어쓰는 것이 아닙니다.\n",
    "그러므로, Tensor를 직접 덮어써 주어야 한다는 것을 기억하세요:\n",
    "``my_tensor = my_tensor.to(torch.device('cuda'))``.\n",
    "\n",
    "## 5. CPU에서 저장하고 GPU (CUDA) 에서 불러오기\n",
    "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "\n",
    "CPU에서 학습하고 저장된 모델을 GPU에서 불러올 때는,``torch.load()``함수의 \n",
    "``map_location``인자를 ``cuda:device_id``로 설정해주세요.\n",
    "그러면 주어진 GPU 장치에 모델이 불러와 집니다.\n",
    "\n",
    "모델의 매개변수 Tensor를 CUDA Tensor로 변환하기 위해,\n",
    "``model.to(torch.device('cuda'))``를 호출해주세요.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CUDA GPU 환경에서만 정상으로 동작합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[warning] cuda is not available\n"
     ]
    }
   ],
   "source": [
    "# 저장하기\n",
    "torch.save(net.state_dict(), PATH)\n",
    "\n",
    "# 불러오기\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    model = Net()\n",
    "    # 사용하고자 하는 GPU 장치 번호를 지정합니다.\n",
    "    model.load_state_dict(torch.load(PATH, map_location=\"cuda:0\"))\n",
    "    # 모델에 사용되는 모든 입력 Tensor들에 대해 input = input.to(device) 을 호출해야 합니다.\n",
    "    model.to(device)\n",
    "else:\n",
    "    print('[warning] cuda is not available')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5-1. CPU에서 저장하고 GPU (M1 Silicon GPU) 에서 불러오기\n",
    "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "\n",
    "CPU에서 학습하고 저장된 모델을 GPU에서 불러올 때는,``torch.load()``함수의 \n",
    "``map_location``인자를 ``cuda:device_id``로 설정해주세요.\n",
    "그러면 주어진 GPU 장치에 모델이 불러와 집니다.\n",
    "\n",
    "모델의 매개변수 Tensor를 CUDA Tensor로 변환하기 위해,\n",
    "``model.to(torch.device('mps'))``를 호출해주세요.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "tensor([1., 1., 1., 1., 1.], device='mps:0')\n",
      "1.12.1\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "import torch\n",
    "print(torch.backends.mps.is_available())\n",
    "mps_device = torch.device(\"mps\")\n",
    "x = torch.ones(5, device=mps_device)\n",
    "print(x)\n",
    "\n",
    "print(torch.__version__) \n",
    "print(torch.backends.mps.is_built())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 현재 오류가 발생하고 있습니다.\n",
    "\n",
    "- 2022년 8월 27일 현재 문제가 해결되지 않은 것으로 보입니다.\n",
    "- 그러나, 관련 이슈가 올라오고 있는 상황이므로 다음버전의 pytorch에서는 해결될 것으로 예상됩니다.\n",
    "- https://github.com/pytorch/pytorch/issues/79384\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "don't know how to restore data location of torch.storage._UntypedStorage (tagged with mps:0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [43]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m model \u001b[38;5;241m=\u001b[39m Net()\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# 사용하고자 하는 GPU 장치 번호를 지정합니다.\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m d \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mPATH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmps:0\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m model\u001b[38;5;241m.\u001b[39mload_state_dict( d )\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# 모델에 사용되는 모든 입력 Tensor들에 대해 input = input.to(device) 을 호출해야 합니다.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/lib/python3.9/site-packages/torch/serialization.py:712\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    710\u001b[0m             opened_file\u001b[38;5;241m.\u001b[39mseek(orig_position)\n\u001b[1;32m    711\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mload(opened_file)\n\u001b[0;32m--> 712\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mopened_zipfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpickle_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpickle_load_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    713\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _legacy_load(opened_file, map_location, pickle_module, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpickle_load_args)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/lib/python3.9/site-packages/torch/serialization.py:1049\u001b[0m, in \u001b[0;36m_load\u001b[0;34m(zip_file, map_location, pickle_module, pickle_file, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1047\u001b[0m unpickler \u001b[38;5;241m=\u001b[39m UnpicklerWrapper(data_file, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpickle_load_args)\n\u001b[1;32m   1048\u001b[0m unpickler\u001b[38;5;241m.\u001b[39mpersistent_load \u001b[38;5;241m=\u001b[39m persistent_load\n\u001b[0;32m-> 1049\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43munpickler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1051\u001b[0m torch\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_validate_loaded_sparse_tensors()\n\u001b[1;32m   1053\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/lib/python3.9/site-packages/torch/serialization.py:1019\u001b[0m, in \u001b[0;36m_load.<locals>.persistent_load\u001b[0;34m(saved_id)\u001b[0m\n\u001b[1;32m   1017\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m loaded_storages:\n\u001b[1;32m   1018\u001b[0m     nbytes \u001b[38;5;241m=\u001b[39m numel \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_element_size(dtype)\n\u001b[0;32m-> 1019\u001b[0m     \u001b[43mload_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_maybe_decode_ascii\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1021\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loaded_storages[key]\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/lib/python3.9/site-packages/torch/serialization.py:1001\u001b[0m, in \u001b[0;36m_load.<locals>.load_tensor\u001b[0;34m(dtype, numel, key, location)\u001b[0m\n\u001b[1;32m    997\u001b[0m storage \u001b[38;5;241m=\u001b[39m zip_file\u001b[38;5;241m.\u001b[39mget_storage_from_record(name, numel, torch\u001b[38;5;241m.\u001b[39m_UntypedStorage)\u001b[38;5;241m.\u001b[39mstorage()\u001b[38;5;241m.\u001b[39m_untyped()\n\u001b[1;32m    998\u001b[0m \u001b[38;5;66;03m# TODO: Once we decide to break serialization FC, we can\u001b[39;00m\n\u001b[1;32m    999\u001b[0m \u001b[38;5;66;03m# stop wrapping with _TypedStorage\u001b[39;00m\n\u001b[1;32m   1000\u001b[0m loaded_storages[key] \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstorage\u001b[38;5;241m.\u001b[39m_TypedStorage(\n\u001b[0;32m-> 1001\u001b[0m     wrap_storage\u001b[38;5;241m=\u001b[39m\u001b[43mrestore_location\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstorage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m   1002\u001b[0m     dtype\u001b[38;5;241m=\u001b[39mdtype)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/lib/python3.9/site-packages/torch/serialization.py:970\u001b[0m, in \u001b[0;36m_get_restore_location.<locals>.restore_location\u001b[0;34m(storage, location)\u001b[0m\n\u001b[1;32m    969\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrestore_location\u001b[39m(storage, location):\n\u001b[0;32m--> 970\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdefault_restore_location\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstorage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/lib/python3.9/site-packages/torch/serialization.py:178\u001b[0m, in \u001b[0;36mdefault_restore_location\u001b[0;34m(storage, location)\u001b[0m\n\u001b[1;32m    176\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    177\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m result\n\u001b[0;32m--> 178\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdon\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt know how to restore data location of \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    179\u001b[0m                    \u001b[38;5;241m+\u001b[39m torch\u001b[38;5;241m.\u001b[39mtypename(storage) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m (tagged with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    180\u001b[0m                    \u001b[38;5;241m+\u001b[39m location \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: don't know how to restore data location of torch.storage._UntypedStorage (tagged with mps:0)"
     ]
    }
   ],
   "source": [
    "# 저장하기\n",
    "torch.save(net.state_dict(), PATH)\n",
    "\n",
    "\n",
    "if torch.backends.mps.is_available():\n",
    "    # 불러오기\n",
    "    device = torch.device(\"mps\")\n",
    "    model = Net()\n",
    "    # 사용하고자 하는 GPU 장치 번호를 지정합니다.\n",
    "    d = torch.load(PATH, map_location=\"mps:0\")\n",
    "    model.load_state_dict( d )\n",
    "    # 모델에 사용되는 모든 입력 Tensor들에 대해 input = input.to(device) 을 호출해야 합니다.\n",
    "    model.to(device)\n",
    "    print(\"loading ok\")\n",
    "else:\n",
    "    print('[warning] mps is not available')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. ``torch.nn.DataParallel`` 모델을 저장하고 불러오기\n",
    "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "\n",
    "``torch.nn.DataParallel``은 병렬 GPU 활용을 가능하게 하는 모델 래퍼(wrapper)입니다.\n",
    "\n",
    "``DataParallel`` 모델을 범용적으로 저장하기 위해서는\n",
    "``model.module.state_dict()``을 사용하면 됩니다.\n",
    "그러면 원하는 장치에 원하는 방식으로 유연하게 모델을 불러올 수 있습니다.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.pt\n"
     ]
    }
   ],
   "source": [
    "print(PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# 저장하기\n",
    "torch.save(net.state_dict(), PATH)\n",
    "\n",
    "# 사용할 장치에 불러오기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "마침"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
