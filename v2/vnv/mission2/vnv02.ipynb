{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f5a09739",
   "metadata": {},
   "source": [
    "# VnV \n",
    "\n",
    "- JPark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e9a78184-a00c-4261-aaf6-8e7efebfdae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#------------------------------------------------------\n",
    "# Config\n",
    "#------------------------------------------------------\n",
    "\n",
    "# Test images\n",
    "\n",
    "zip_images_url = 'http://keticmr.iptime.org:22080/edgeai/images/imagenet-mini-val.zip'\n",
    "zip_images = 'imagenet-mini-val.zip'\n",
    "dataset_root = './dataset'\n",
    "fpath_zip_images = dataset_root + '/' + zip_images\n",
    "fpath_testimages = dataset_root + '/imagenet-mini-val/'\n",
    "\n",
    "# Models\n",
    "#names = ['res18_weights.pth', 'res34_weights.pth', 'res50_weights.pth', 'res101_weights.pth', 'res152_weights.pth']\n",
    "names = ['resnet18-dict.pth', 'resnet34-dict.pth', 'resnet50-dict.pth', 'resnet101-dict.pth', 'resnet152-dict.pth']\n",
    "urlroot = 'http://keticmr.iptime.org:22080/edgeai/models_jpark/'\n",
    "modeldir = './checkpoint/'\n",
    "\n",
    "urlmodels = []\n",
    "for name in names:\n",
    "    urlmodels.append( urlroot + name )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "745976a9-34c7-443b-a284-afc572efbf5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#------------------------------------------------------\n",
    "# Download data\n",
    "#------------------------------------------------------\n",
    "import os\n",
    "# make download directory\n",
    "def makedir(path): \n",
    "    try: \n",
    "        os.makedirs(path)\n",
    "    except OSError: \n",
    "        if not os.path.isdir(path): \n",
    "            raise\n",
    "    return os.path.abspath(path)\n",
    "\n",
    "# Download images\n",
    "import urllib, os\n",
    "\n",
    "d = makedir(dataset_root) # 저장 공간 생성\n",
    "url, fname = (zip_images_url, fpath_zip_images)\n",
    "try: \n",
    "    urllib.URLopener().retrieve(url, fname)\n",
    "except: \n",
    "    urllib.request.urlretrieve(url, fname)\n",
    "    \n",
    "\n",
    "# Unzip\n",
    "cmd = 'unzip ' + fpath_zip_images + ' -d ' + dataset_root\n",
    "print(cmd)\n",
    "os.system(cmd)\n",
    "\n",
    "# download models\n",
    "makedir(modeldir)\n",
    "\n",
    "for name in names:\n",
    "    torch.hub.download_url_to_file(urlroot+name, modeldir+name)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e73d6b4-49f6-4879-8618-0e0e3ce8fee6",
   "metadata": {},
   "source": [
    "## Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2e2e5985-bf26-42b2-b782-26a4b09adcae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /Users/jpark/.cache/torch/hub/pytorch_vision_v0.10.0\n",
      "Using cache found in /Users/jpark/.cache/torch/hub/pytorch_vision_v0.10.0\n",
      "Using cache found in /Users/jpark/.cache/torch/hub/pytorch_vision_v0.10.0\n",
      "Using cache found in /Users/jpark/.cache/torch/hub/pytorch_vision_v0.10.0\n",
      "Using cache found in /Users/jpark/.cache/torch/hub/pytorch_vision_v0.10.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "#------------------------------------------------------\n",
    "# Import packages\n",
    "#------------------------------------------------------\n",
    "import torch\n",
    "\n",
    "\n",
    "model_fpaths = []\n",
    "for name in names:\n",
    "    model_fpaths.append(modeldir + name)\n",
    "\n",
    "model_fpaths\n",
    "#print(model_fpaths)\n",
    "#print(urlroot+name)\n",
    "#print(modeldir+name)\n",
    "\n",
    "\n",
    "#------------------------------------------------------\n",
    "# initial models\n",
    "#------------------------------------------------------\n",
    "\n",
    "model_names = ['resnet18', 'resnet34', 'resnet50', 'resnet101', 'resnet152']\n",
    "models = []\n",
    "\n",
    "for model_name in model_names:\n",
    "    m = torch.hub.load('pytorch/vision:v0.10.0', model_name, pretrained=False)\n",
    "    models.append(m)\n",
    "    m.eval()\n",
    "    \n",
    "# load saved weigts to the initial model\n",
    "for idx, model in enumerate(models):\n",
    "    checkpoint = urlmodels[idx]\n",
    "    model.load_state_dict(torch.hub.load_state_dict_from_url(checkpoint, progress=False))\n",
    "    model.eval()\n",
    "    print(idx)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d77f07d7-965a-4646-875a-95c22f62b671",
   "metadata": {},
   "source": [
    "## Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d1703d30-b54d-4ba2-8f8c-c9471f0c8207",
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import iglob\n",
    "\n",
    "'''\n",
    "# read test files\n",
    "testfiles = []\n",
    "for fname in sorted( iglob(fpath_testimages + '**/*.JPEG', recursive=True) ):\n",
    "    testfiles.append(fname)\n",
    "'''\n",
    "\n",
    "idx_gt = []\n",
    "idx = 0\n",
    "testfiles = []\n",
    "for d in sorted( iglob(fpath_testimages + 'n*', recursive=False) ):\n",
    "    for fname in sorted( iglob(d + '/*.JPEG', recursive=True) ):\n",
    "        testfiles.append(fname)\n",
    "        idx_gt.append( idx )\n",
    "    idx += 1\n",
    "\n",
    "\n",
    "# Read the categories\n",
    "with open(\"imagenet_classes.txt\", \"r\") as f:\n",
    "    categories = [s.strip() for s in f.readlines()]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "754a0e23-769e-4a44-94e1-e6dc28c60157",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['resnet18', 'resnet34', 'resnet50', 'resnet101', 'resnet152']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "796b3e54-8600-410e-b26e-0bd439eb8b50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "resnet18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████| 100/100 [00:06<00:00, 15.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n =  100\n",
      "top1_cnt =  98\n",
      "top1_cnt/n =  0.98\n",
      "time =  6.4047300815582275\n",
      "resnet34\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████| 100/100 [00:11<00:00,  9.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n =  100\n",
      "top1_cnt =  98\n",
      "top1_cnt/n =  0.98\n",
      "time =  11.058824300765991\n",
      "resnet50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████| 100/100 [00:13<00:00,  7.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n =  100\n",
      "top1_cnt =  99\n",
      "top1_cnt/n =  0.99\n",
      "time =  13.026250123977661\n",
      "resnet101\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████| 100/100 [00:21<00:00,  4.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n =  100\n",
      "top1_cnt =  100\n",
      "top1_cnt/n =  1.0\n",
      "time =  21.852193117141724\n",
      "resnet152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████| 100/100 [00:30<00:00,  3.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n =  100\n",
      "top1_cnt =  99\n",
      "top1_cnt/n =  0.99\n",
      "time =  30.642608880996704\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# sample execution (requires torchvision)\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "'''\n",
    "# Define transforms for the evaluation phase\n",
    "preprocess = transforms.Compose([transforms.Resize(256),\n",
    "                                      transforms.CenterCrop(224),\n",
    "                                      transforms.ToTensor(),\n",
    "                                      transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "                                ])\n",
    "'''\n",
    "\n",
    "for i, model in enumerate(models):\n",
    "    print(model_names[i])\n",
    "    \n",
    "    top1_cnt = 0\n",
    "    top5_cnt = 0\n",
    "\n",
    "    testset = testfiles[:100]\n",
    "    n = len(testset)\n",
    "    idx = 0\n",
    "    \n",
    "    start = time.time()\n",
    "\n",
    "\n",
    "    preprocess = transforms.Compose([\n",
    "            transforms.Resize(256),\n",
    "            transforms.CenterCrop(224),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "\n",
    "    for fpath in tqdm( testset ):\n",
    "        #print( fpath )\n",
    "        input_image = Image.open(fpath)\n",
    "\n",
    "        try:\n",
    "            input_tensor = preprocess(input_image)\n",
    "        except:\n",
    "            #print(idx)\n",
    "            # gray scale to color\n",
    "            input_image = Image.open(fpath).convert(\"RGB\")\n",
    "            input_tensor = preprocess(input_image)\n",
    "\n",
    "        input_batch = input_tensor.unsqueeze(0) # create a mini-batch as expected by the model\n",
    "\n",
    "        # move the input and model to GPU for speed if available\n",
    "        if torch.cuda.is_available():\n",
    "            input_batch = input_batch.to('cuda')\n",
    "            model.to('cuda')\n",
    "        else:\n",
    "            model.to('cpu')\n",
    "    \n",
    "\n",
    "        with torch.no_grad():\n",
    "            output = model(input_batch)\n",
    "\n",
    "        # Tensor of shape 1000, with confidence scores over Imagenet's 1000 classes\n",
    "        #print(output[0])\n",
    "\n",
    "        # The output has unnormalized scores. To get probabilities, you can run a softmax on it.\n",
    "        probabilities = torch.nn.functional.softmax(output[0], dim=0)\n",
    "        #print(probabilities)\n",
    "\n",
    "    \n",
    "        # Show top categories per image\n",
    "        top5_prob, top5_catid = torch.topk(probabilities, 5)\n",
    "        for i in range(top5_prob.size(0)):\n",
    "            #print(top5_catid[i])\n",
    "            #pass\n",
    "            #print(idx, ' ', categories[top5_catid[i]], top5_prob[i].item())\n",
    "\n",
    "            if( top5_catid[i] == idx_gt[idx] ):\n",
    "                top1_cnt += 1\n",
    "\n",
    "        #print('')\n",
    "        idx += 1\n",
    "\n",
    "    end = time.time()\n",
    "    print('n = ', n)\n",
    "    print('top1_cnt = ', top1_cnt)\n",
    "    print('top1_cnt/n = ', top1_cnt/n)\n",
    "    print('time = ', end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d02d211f-ecec-4e87-b7ec-c510cc0e6d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "n =  3923\n",
    "top1_cnt =  3519\n",
    "top1_cnt/n =  0.8970175885801682\n",
    "time =  99.26183104515076"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71abef25-1608-4e42-884f-cd099daaa740",
   "metadata": {},
   "outputs": [],
   "source": [
    "n =  3923\n",
    "top1_cnt =  3299\n",
    "top1_cnt/n =  0.8409380576089728\n",
    "time =  37.87246131896973"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f265d22a-a55b-49e2-8fe2-c5c6a117e6b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet18\n",
    "\n",
    "n =  3923\n",
    "top1_cnt =  3482\n",
    "top1_cnt/n =  0.887586031098649\n",
    "time =  429.7253770828247"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "843e599f-52f1-497e-b1d7-51b2206ca744",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e21433d7-28c4-456b-a63a-9ecb6bf4c7bf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base(conda)",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
