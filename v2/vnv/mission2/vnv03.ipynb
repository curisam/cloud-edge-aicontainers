{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f5a09739",
   "metadata": {},
   "source": [
    "# VnV (ver. 3)\n",
    "\n",
    "- by JPark\n",
    "- 모델 추가\n",
    "- 지연 시간 측정을 위한 코드 통합"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e9a78184-a00c-4261-aaf6-8e7efebfdae1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jpark/.local/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#------------------------------------------------------\n",
    "# Config\n",
    "#------------------------------------------------------\n",
    "\n",
    "# Test images\n",
    "\n",
    "zip_images_url = 'http://keticmr.iptime.org:22080/edgeai/images/imagenet-mini-val.zip'\n",
    "zip_images = 'imagenet-mini-val.zip'\n",
    "dataset_root = './dataset'\n",
    "fpath_zip_images = dataset_root + '/' + zip_images\n",
    "fpath_testimages = dataset_root + '/imagenet-mini-val/'\n",
    "\n",
    "# Models\n",
    "urlroot = 'http://keticmr.iptime.org:22080/edgeai/models_jpark/'\n",
    "modeldir = './checkpoint/'\n",
    "    \n",
    "model_names_resnet = [ \n",
    "    'resnet18',\n",
    "    'resnet34',\n",
    "    'resnet50',\n",
    "    'resnet101',\n",
    "    'resnet152',\n",
    "]\n",
    "repo_resnet = 'pytorch/vision:v0.10.0'\n",
    "\n",
    "model_names_mobnet = [\n",
    "    'mobilenet_v3_small',\n",
    "    'mobilenet_v3_large',\n",
    "]\n",
    "repo_mobnet = 'pytorch/vision:v0.10.0'\n",
    "\n",
    "'''\n",
    "model_names_effnet = [ \n",
    "    'nvidia_efficientnet_b0',\n",
    "    'nvidia_efficientnet_b4',\n",
    "    'nvidia_efficientnet_widese_b0',\n",
    "    'nvidia_efficientnet_widese_b4',\n",
    "]\n",
    "repo_effnet = 'NVIDIA/DeepLearningExamples:torchhub'\n",
    "'''\n",
    "\n",
    "from efficientnet_pytorch import EfficientNet\n",
    "model_names_effnet = [ \n",
    "    'efficientnet-b0',\n",
    "    'efficientnet-b1',\n",
    "    'efficientnet-b2',\n",
    "    'efficientnet-b3',\n",
    "    'efficientnet-b4',\n",
    "    'efficientnet-b5',\n",
    "    'efficientnet-b6',\n",
    "    'efficientnet-b7',\n",
    "]\n",
    "repo_effnet = ''\n",
    "\n",
    "\n",
    "\n",
    "model_names = model_names_resnet + model_names_effnet + model_names_mobnet\n",
    "#model_names = model_names_effnet\n",
    "\n",
    "\n",
    "pth_names = [ model_name + '-dict.pth' for model_name in model_names ]\n",
    "\n",
    "urlmodels = []\n",
    "for pth_name in pth_names:\n",
    "    urlmodels.append(urlroot + pth_name)\n",
    "\n",
    "model_fpaths = []\n",
    "for pth_name in pth_names:\n",
    "    model_fpaths.append(modeldir + pth_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "59c318fe-450d-4040-b263-2de5925379f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['resnet18-dict.pth',\n",
       " 'resnet34-dict.pth',\n",
       " 'resnet50-dict.pth',\n",
       " 'resnet101-dict.pth',\n",
       " 'resnet152-dict.pth',\n",
       " 'efficientnet-b0-dict.pth',\n",
       " 'efficientnet-b1-dict.pth',\n",
       " 'efficientnet-b2-dict.pth',\n",
       " 'efficientnet-b3-dict.pth',\n",
       " 'efficientnet-b4-dict.pth',\n",
       " 'efficientnet-b5-dict.pth',\n",
       " 'efficientnet-b6-dict.pth',\n",
       " 'efficientnet-b7-dict.pth',\n",
       " 'mobilenet_v3_small-dict.pth',\n",
       " 'mobilenet_v3_large-dict.pth']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pth_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "baaa203c-01ec-49d8-ad82-fe90b6be9cdf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['http://keticmr.iptime.org:22080/edgeai/models_jpark/resnet18-dict.pth',\n",
       " 'http://keticmr.iptime.org:22080/edgeai/models_jpark/resnet34-dict.pth',\n",
       " 'http://keticmr.iptime.org:22080/edgeai/models_jpark/resnet50-dict.pth',\n",
       " 'http://keticmr.iptime.org:22080/edgeai/models_jpark/resnet101-dict.pth',\n",
       " 'http://keticmr.iptime.org:22080/edgeai/models_jpark/resnet152-dict.pth',\n",
       " 'http://keticmr.iptime.org:22080/edgeai/models_jpark/efficientnet-b0-dict.pth',\n",
       " 'http://keticmr.iptime.org:22080/edgeai/models_jpark/efficientnet-b1-dict.pth',\n",
       " 'http://keticmr.iptime.org:22080/edgeai/models_jpark/efficientnet-b2-dict.pth',\n",
       " 'http://keticmr.iptime.org:22080/edgeai/models_jpark/efficientnet-b3-dict.pth',\n",
       " 'http://keticmr.iptime.org:22080/edgeai/models_jpark/efficientnet-b4-dict.pth',\n",
       " 'http://keticmr.iptime.org:22080/edgeai/models_jpark/efficientnet-b5-dict.pth',\n",
       " 'http://keticmr.iptime.org:22080/edgeai/models_jpark/efficientnet-b6-dict.pth',\n",
       " 'http://keticmr.iptime.org:22080/edgeai/models_jpark/efficientnet-b7-dict.pth',\n",
       " 'http://keticmr.iptime.org:22080/edgeai/models_jpark/mobilenet_v3_small-dict.pth',\n",
       " 'http://keticmr.iptime.org:22080/edgeai/models_jpark/mobilenet_v3_large-dict.pth']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "urlmodels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8d3a1026-1408-4aab-9ae7-6c75647e1295",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./checkpoint/resnet18-dict.pth',\n",
       " './checkpoint/resnet34-dict.pth',\n",
       " './checkpoint/resnet50-dict.pth',\n",
       " './checkpoint/resnet101-dict.pth',\n",
       " './checkpoint/resnet152-dict.pth',\n",
       " './checkpoint/efficientnet-b0-dict.pth',\n",
       " './checkpoint/efficientnet-b1-dict.pth',\n",
       " './checkpoint/efficientnet-b2-dict.pth',\n",
       " './checkpoint/efficientnet-b3-dict.pth',\n",
       " './checkpoint/efficientnet-b4-dict.pth',\n",
       " './checkpoint/efficientnet-b5-dict.pth',\n",
       " './checkpoint/efficientnet-b6-dict.pth',\n",
       " './checkpoint/efficientnet-b7-dict.pth',\n",
       " './checkpoint/mobilenet_v3_small-dict.pth',\n",
       " './checkpoint/mobilenet_v3_large-dict.pth']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_fpaths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a75e3e43-34e4-4ee6-9ac0-321f6889f08c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./dataset/imagenet-mini-val.zip'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fpath_zip_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bb0e7faf-8908-4c89-adb2-acca19e3e727",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['resnet18',\n",
       " 'resnet34',\n",
       " 'resnet50',\n",
       " 'resnet101',\n",
       " 'resnet152',\n",
       " 'efficientnet-b0',\n",
       " 'efficientnet-b1',\n",
       " 'efficientnet-b2',\n",
       " 'efficientnet-b3',\n",
       " 'efficientnet-b4',\n",
       " 'efficientnet-b5',\n",
       " 'efficientnet-b6',\n",
       " 'efficientnet-b7',\n",
       " 'mobilenet_v3_small',\n",
       " 'mobilenet_v3_large']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7404f9e9-af78-4415-b385-99c520407e30",
   "metadata": {},
   "source": [
    "## 00. 사전작업\n",
    "\n",
    "- 사전으로 실험을 위한 디렉토리를 생성하고, 추론 영상을 다운로드 받습니다. (변인통제 요소)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "745976a9-34c7-443b-a284-afc572efbf5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downloading...\n",
      "[+] download skipped \n"
     ]
    }
   ],
   "source": [
    "\n",
    "#------------------------------------------------------\n",
    "# Download data\n",
    "#------------------------------------------------------\n",
    "import os\n",
    "import urllib\n",
    "\n",
    "# make download directory\n",
    "def makedir(path): \n",
    "    isdir = os.path.isdir(path)\n",
    "    \n",
    "    try: \n",
    "        os.makedirs(path)\n",
    "    except OSError: \n",
    "        if not isdir: \n",
    "            raise\n",
    "    return os.path.abspath(path), isdir\n",
    "\n",
    "# Download images\n",
    "d, isdir = makedir(dataset_root) # 저장 공간 생성\n",
    "\n",
    "if isdir:\n",
    "    url, fname = (zip_images_url, fpath_zip_images)\n",
    "    isfile_exist = os.path.exists(os.path.join(os.getcwd(), fname))\n",
    "    \n",
    "    print('downloading...')\n",
    "    if not isfile_exist:\n",
    "        try: \n",
    "            urllib.URLopener().retrieve(url, fname)\n",
    "        except: \n",
    "            urllib.request.urlretrieve(url, fname)\n",
    "        print('[+] download completed.')\n",
    "        # Unzip\n",
    "        cmd = 'unzip ' + fpath_zip_images + ' -d ' + dataset_root\n",
    "        print(cmd)\n",
    "        os.system(cmd)\n",
    "    else:\n",
    "        print('[+] download skipped ')\n",
    "\n",
    "\n",
    "from glob import iglob\n",
    "\n",
    "'''\n",
    "# read test files\n",
    "testfiles = []\n",
    "for fname in sorted( iglob(fpath_testimages + '**/*.JPEG', recursive=True) ):\n",
    "    testfiles.append(fname)\n",
    "'''\n",
    "\n",
    "idx_gt = []\n",
    "idx = 0\n",
    "testfiles = []\n",
    "for d in sorted( iglob(fpath_testimages + 'n*', recursive=False) ):\n",
    "    for fname in sorted( iglob(d + '/*.JPEG', recursive=True) ):\n",
    "        testfiles.append(fname)\n",
    "        idx_gt.append( idx )\n",
    "    idx += 1\n",
    "\n",
    "\n",
    "# Read the categories\n",
    "with open(\"imagenet_classes.txt\", \"r\") as f:\n",
    "    categories = [s.strip() for s in f.readlines()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a6c0de-5ddb-4ba3-87c3-f4e018048b7c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2e73d6b4-49f6-4879-8618-0e0e3ce8fee6",
   "metadata": {},
   "source": [
    "## 01. 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "51491624-9b98-44af-8d7f-2d02fd2e238b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3923"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(idx_gt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3870e27e-5639-4149-aa75-4e61e2b0d90f",
   "metadata": {},
   "source": [
    "## GPU 동작모드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "796b3e54-8600-410e-b26e-0bd439eb8b50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "device =  cuda\n",
      "--------------------------------------------------\n",
      "model = resnet18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/jpark/.cache/torch/hub/pytorch_vision_v0.10.0\n",
      "/home/jpark/.local/lib/python3.8/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/jpark/.local/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Unexpected error from cudaGetDeviceCount(). Did you run some cuda functions before calling NumCudaDevices() that might have already set an error? Error 804: forward compatibility was attempted on non supported HW",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [9], line 63\u001b[0m\n\u001b[1;32m     61\u001b[0m     checkpoint \u001b[38;5;241m=\u001b[39m urlmodels[model_idx]\n\u001b[1;32m     62\u001b[0m     model\u001b[38;5;241m.\u001b[39mload_state_dict(torch\u001b[38;5;241m.\u001b[39mhub\u001b[38;5;241m.\u001b[39mload_state_dict_from_url(checkpoint, progress\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m))\n\u001b[0;32m---> 63\u001b[0m     \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meval\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# change model to evauation mode (e.g. disable Dropout, BatchNorm)\u001b[39;00m\n\u001b[1;32m     64\u001b[0m     models\u001b[38;5;241m.\u001b[39mappend(model)\n\u001b[1;32m     66\u001b[0m \u001b[38;5;66;03m# 시험용 입력 영상\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:927\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    923\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    924\u001b[0m                     non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[1;32m    925\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, non_blocking)\n\u001b[0;32m--> 927\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:579\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    577\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn):\n\u001b[1;32m    578\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 579\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    581\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    582\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    583\u001b[0m             \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    584\u001b[0m             \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    589\u001b[0m             \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    590\u001b[0m             \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:602\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    598\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    599\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    600\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    601\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 602\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    603\u001b[0m should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    604\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m should_use_set_data:\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:925\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    922\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m    923\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    924\u001b[0m                 non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[0;32m--> 925\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/cuda/__init__.py:217\u001b[0m, in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    213\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\n\u001b[1;32m    214\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlibcudart functions unavailable. It looks like you have a broken build?\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    215\u001b[0m \u001b[38;5;66;03m# This function throws if there's a driver initialization error, no GPUs\u001b[39;00m\n\u001b[1;32m    216\u001b[0m \u001b[38;5;66;03m# are found or any other error occurs\u001b[39;00m\n\u001b[0;32m--> 217\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cuda_init\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m \u001b[38;5;66;03m# Some of the queued calls may reentrantly call _lazy_init();\u001b[39;00m\n\u001b[1;32m    219\u001b[0m \u001b[38;5;66;03m# we need to just return without initializing in that case.\u001b[39;00m\n\u001b[1;32m    220\u001b[0m \u001b[38;5;66;03m# However, we must not let any *other* threads in!\u001b[39;00m\n\u001b[1;32m    221\u001b[0m _tls\u001b[38;5;241m.\u001b[39mis_initializing \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Unexpected error from cudaGetDeviceCount(). Did you run some cuda functions before calling NumCudaDevices() that might have already set an error? Error 804: forward compatibility was attempted on non supported HW"
     ]
    }
   ],
   "source": [
    "# sample execution (requires torchvision)\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "'''\n",
    "# Define transforms for the evaluation phase\n",
    "preprocess = transforms.Compose([transforms.Resize(256),\n",
    "                                      transforms.CenterCrop(224),\n",
    "                                      transforms.ToTensor(),\n",
    "                                      transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "                                ])\n",
    "'''\n",
    "\n",
    "preproc = ['method1', 'method2']\n",
    "preproc_method = 'method1'\n",
    "\n",
    "#devices = ['cuda', 'cpu']\n",
    "devices = ['cuda']\n",
    "\n",
    "models = []\n",
    "\n",
    "testset = testfiles[:]\n",
    "n = len(testset)\n",
    "\n",
    "# 디바이스별 반복\n",
    "for device in devices: \n",
    "    print('-'*50)\n",
    "    print('device = ', device, flush=True)\n",
    "    print('-'*50)\n",
    "        \n",
    "    # 모델별 반복\n",
    "    for model_idx, model_name in enumerate(model_names):\n",
    "        start = time.time() # strt timer        \n",
    "        print(f'model = {model_names[model_idx]}', flush=True)\n",
    "        \n",
    "        # 모델 템플릿 다운로드 (from torch.hub)\n",
    "        if model_name in model_names_resnet:\n",
    "            model = torch.hub.load(repo_resnet, model_name, pretrained=False)\n",
    "        elif model_name in model_names_mobnet:\n",
    "            model = torch.hub.load(repo_mobnet, model_name, pretrained=False)\n",
    "        elif model_name in model_names_effnet:\n",
    "            #model = torch.hub.load(repo_effnet, model_name, pretrained=False)\n",
    "            model = EfficientNet.from_pretrained(model_name)\n",
    "        else:\n",
    "            pass\n",
    "    \n",
    "        # 모델 가중치 다운로드 (from AI 모델 리포지토리)\n",
    "        if False:    \n",
    "            # 방법 1\n",
    "            pass\n",
    "        \n",
    "            # torch.hub.download_url_to_file(urlroot+pthnames[model_idx], modeldir+pthnames[model_idx])\n",
    "            # model.load_state_dict(torch.hub.load_state_dict_from_url(checkpoint, progress=False))\n",
    "            # model.eval()\n",
    "        else:\n",
    "            # 방법 2\n",
    "            checkpoint = urlmodels[model_idx]\n",
    "            model.load_state_dict(torch.hub.load_state_dict_from_url(checkpoint, progress=False))\n",
    "            model.eval().to(device) # change model to evauation mode (e.g. disable Dropout, BatchNorm)\n",
    "            models.append(model)\n",
    " \n",
    "        # 시험용 입력 영상\n",
    "        top1_cnt = 0\n",
    "        top5_cnt = 0\n",
    "              \n",
    "        # 시험용 입력 영상 전처리 (크기 및 컬러채널)\n",
    "        if preproc_method == preproc[0]:\n",
    "            preprocess = transforms.Compose([\n",
    "                transforms.Resize(256),\n",
    "                transforms.CenterCrop(224),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "            ])\n",
    "        else:\n",
    "            preprocess = transforms.Compose([\n",
    "                transforms.Resize(256),\n",
    "                transforms.CenterCrop(224),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "            ])\n",
    "\n",
    "        # 시험영상별 반복\n",
    "        imgidx = 0\n",
    "        for fpath in tqdm( testset ):\n",
    "        #for fpath in testset:\n",
    "            #print( fpath )\n",
    "            input_image = Image.open(fpath)\n",
    "\n",
    "            try:\n",
    "                input_tensor = preprocess(input_image)\n",
    "            except:\n",
    "                # gray scale to color\n",
    "                input_image = Image.open(fpath).convert(\"RGB\")\n",
    "                input_tensor = preprocess(input_image)\n",
    "\n",
    "            input_batch = input_tensor.unsqueeze(0) # create a mini-batch as expected by the model\n",
    "\n",
    "            # 디바이스 설정\n",
    "            if device == 'cuda':\n",
    "                if torch.cuda.is_available():\n",
    "                    input_batch = input_batch.to('cuda')\n",
    "                    model.to('cuda')\n",
    "            else:\n",
    "                input_batch = input_batch.to('cpu')\n",
    "                model.to('cpu')\n",
    "              \n",
    "            with torch.no_grad():\n",
    "                output = model(input_batch)\n",
    "\n",
    "            # Tensor of shape 1000, with confidence scores over Imagenet's 1000 classes\n",
    "            #print(output[0])\n",
    "\n",
    "            # The output has unnormalized scores. To get probabilities, you can run a softmax on it.\n",
    "            probabilities = torch.nn.functional.softmax(output[0], dim=0)\n",
    "            #print(probabilities)\n",
    "\n",
    "            # Show top categories per image\n",
    "            top5_prob, top5_catid = torch.topk(probabilities, 5)\n",
    "            for i in range(top5_prob.size(0)):\n",
    "                #print(top5_catid[i])\n",
    "                #print(imgidx, ' ', categories[top5_catid[i]], top5_prob[i].item())\n",
    "\n",
    "                if( top5_catid[i] == idx_gt[imgidx] ):\n",
    "                    top5_cnt += 1\n",
    "                    \n",
    "            # Show top categories per image\n",
    "            top1_prob, top1_catid = torch.topk(probabilities, 1)\n",
    "            if( top1_catid[0] == idx_gt[imgidx] ):\n",
    "                top1_cnt += 1\n",
    " \n",
    "            imgidx += 1\n",
    "\n",
    "        end = time.time() # end timer\n",
    "        print('n = ', n)\n",
    "        print('top1_cnt = ', top1_cnt)\n",
    "        print('top1_acc = ', top1_cnt/n)\n",
    "        print('top5_cnt = ', top5_cnt)\n",
    "        print('top5_acc = ', top5_cnt/n)\n",
    "        print('time = ', end - start)\n",
    "        print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16dbe875-c290-44bb-aaab-894026d71fec",
   "metadata": {},
   "source": [
    "## CPU 동작모드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "44089a94-ce28-490e-bcd3-f8ae00b89346",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "device =  cpu\n",
      "--------------------------------------------------\n",
      "model = resnet18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/jpark/.cache/torch/hub/pytorch_vision_v0.10.0\n",
      "100%|██████████| 3923/3923 [01:55<00:00, 33.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n =  3923\n",
      "top1_cnt =  3358\n",
      "top1_cnt/n =  0.8559775681876115\n",
      "time =  116.17684650421143\n",
      "\n",
      "model = resnet34\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Using cache found in /home/jpark/.cache/torch/hub/pytorch_vision_v0.10.0\n",
      "100%|██████████| 3923/3923 [03:01<00:00, 21.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n =  3923\n",
      "top1_cnt =  3476\n",
      "top1_cnt/n =  0.8860565893448891\n",
      "time =  182.05406522750854\n",
      "\n",
      "model = resnet50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Using cache found in /home/jpark/.cache/torch/hub/pytorch_vision_v0.10.0\n",
      "100%|██████████| 3923/3923 [03:44<00:00, 17.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n =  3923\n",
      "top1_cnt =  3545\n",
      "top1_cnt/n =  0.9036451695131277\n",
      "time =  224.42821073532104\n",
      "\n",
      "model = resnet101\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Using cache found in /home/jpark/.cache/torch/hub/pytorch_vision_v0.10.0\n",
      "100%|██████████| 3923/3923 [06:06<00:00, 10.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n =  3923\n",
      "top1_cnt =  3572\n",
      "top1_cnt/n =  0.9105276574050472\n",
      "time =  367.11171436309814\n",
      "\n",
      "model = resnet152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Using cache found in /home/jpark/.cache/torch/hub/pytorch_vision_v0.10.0\n",
      "100%|██████████| 3923/3923 [08:18<00:00,  7.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n =  3923\n",
      "top1_cnt =  3584\n",
      "top1_cnt/n =  0.9135865409125669\n",
      "time =  499.18171405792236\n",
      "\n",
      "model = nvidia_efficientnet_b0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Using cache found in /home/jpark/.cache/torch/hub/NVIDIA_DeepLearningExamples_torchhub\n",
      "100%|██████████| 3923/3923 [01:52<00:00, 34.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n =  3923\n",
      "top1_cnt =  3619\n",
      "top1_cnt/n =  0.9225082844761662\n",
      "time =  112.36311888694763\n",
      "\n",
      "model = nvidia_efficientnet_b4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Using cache found in /home/jpark/.cache/torch/hub/NVIDIA_DeepLearningExamples_torchhub\n",
      "100%|██████████| 3923/3923 [03:55<00:00, 16.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n =  3923\n",
      "top1_cnt =  3550\n",
      "top1_cnt/n =  0.9049197043079276\n",
      "time =  236.33543014526367\n",
      "\n",
      "model = nvidia_efficientnet_widese_b0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Using cache found in /home/jpark/.cache/torch/hub/NVIDIA_DeepLearningExamples_torchhub\n",
      "100%|██████████| 3923/3923 [01:52<00:00, 34.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n =  3923\n",
      "top1_cnt =  3633\n",
      "top1_cnt/n =  0.9260769819016059\n",
      "time =  113.0606951713562\n",
      "\n",
      "model = nvidia_efficientnet_widese_b4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Using cache found in /home/jpark/.cache/torch/hub/NVIDIA_DeepLearningExamples_torchhub\n",
      "100%|██████████| 3923/3923 [04:01<00:00, 16.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n =  3923\n",
      "top1_cnt =  3516\n",
      "top1_cnt/n =  0.8962528677032883\n",
      "time =  242.02179408073425\n",
      "\n",
      "model = mobilenet_v3_small\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Using cache found in /home/jpark/.cache/torch/hub/pytorch_vision_v0.10.0\n",
      "100%|██████████| 3923/3923 [00:59<00:00, 66.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n =  3923\n",
      "top1_cnt =  3353\n",
      "top1_cnt/n =  0.8547030333928116\n",
      "time =  59.20183300971985\n",
      "\n",
      "model = mobilenet_v3_large\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Using cache found in /home/jpark/.cache/torch/hub/pytorch_vision_v0.10.0\n",
      "100%|██████████| 3923/3923 [01:24<00:00, 46.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n =  3923\n",
      "top1_cnt =  3556\n",
      "top1_cnt/n =  0.9064491460616875\n",
      "time =  84.20090246200562\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# sample execution (requires torchvision)\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "'''\n",
    "# Define transforms for the evaluation phase\n",
    "preprocess = transforms.Compose([transforms.Resize(256),\n",
    "                                      transforms.CenterCrop(224),\n",
    "                                      transforms.ToTensor(),\n",
    "                                      transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "                                ])\n",
    "'''\n",
    "\n",
    "preproc = ['method1', 'method2']\n",
    "preproc_method = 'method1'\n",
    "\n",
    "devices = ['cuda', 'cpu']\n",
    "devices = ['cpu']\n",
    "\n",
    "models = []\n",
    "\n",
    "\n",
    "testset = testfiles[:]\n",
    "n = len(testset)\n",
    "\n",
    "# 디바이스별 반복\n",
    "for device in devices: \n",
    "    print('-'*50)\n",
    "    print('device = ', device, flush=True)\n",
    "    print('-'*50)\n",
    "        \n",
    "    # 모델별 반복\n",
    "    for model_idx, model_name in enumerate(model_names):\n",
    "        start = time.time() # strt timer        \n",
    "        print(f'model = {model_names[model_idx]}', flush=True)\n",
    "        \n",
    "        # 모델 템플릿 다운로드 (from torch.hub)\n",
    "        if model_name in model_names_resnet:\n",
    "            model = torch.hub.load(repo_resnet, model_name, pretrained=False)\n",
    "        elif model_name in model_names_mobnet:\n",
    "            model = torch.hub.load(repo_mobnet, model_name, pretrained=False)\n",
    "        elif model_name in model_names_effnet:\n",
    "            model = torch.hub.load(repo_effnet, model_name, pretrained=False)\n",
    "        else:\n",
    "            pass\n",
    "    \n",
    "        # 모델 가중치 다운로드 (from AI 모델 리포지토리)\n",
    "        if False:    \n",
    "            # 방법 1\n",
    "            pass\n",
    "        \n",
    "            # torch.hub.download_url_to_file(urlroot+pthnames[model_idx], modeldir+pthnames[model_idx])\n",
    "            # model.load_state_dict(torch.hub.load_state_dict_from_url(checkpoint, progress=False))\n",
    "            # model.eval()\n",
    "        else:\n",
    "            # 방법 2\n",
    "            checkpoint = urlmodels[model_idx]\n",
    "            model.load_state_dict(torch.hub.load_state_dict_from_url(checkpoint, progress=False))\n",
    "            model.eval().to(device) # change model to evauation mode (e.g. disable Dropout, BatchNorm)\n",
    "            models.append(model)\n",
    " \n",
    "        # 시험용 입력 영상\n",
    "        top1_cnt = 0\n",
    "        top5_cnt = 0\n",
    "              \n",
    "        # 시험용 입력 영상 전처리 (크기 및 컬러채널)\n",
    "        if preproc_method == preproc[0]:\n",
    "            preprocess = transforms.Compose([\n",
    "                transforms.Resize(256),\n",
    "                transforms.CenterCrop(224),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "            ])\n",
    "        else:\n",
    "            preprocess = transforms.Compose([\n",
    "                transforms.Resize(256),\n",
    "                transforms.CenterCrop(224),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "            ])\n",
    "\n",
    "        # 시험영상별 반복\n",
    "        imgidx = 0\n",
    "        for fpath in tqdm( testset ):\n",
    "        #for fpath in testset:\n",
    "            #print( fpath )\n",
    "            input_image = Image.open(fpath)\n",
    "\n",
    "            try:\n",
    "                input_tensor = preprocess(input_image)\n",
    "            except:\n",
    "                # gray scale to color\n",
    "                input_image = Image.open(fpath).convert(\"RGB\")\n",
    "                input_tensor = preprocess(input_image)\n",
    "\n",
    "            input_batch = input_tensor.unsqueeze(0) # create a mini-batch as expected by the model\n",
    "\n",
    "            # 디바이스 설정\n",
    "            if device == 'cuda':\n",
    "                if torch.cuda.is_available():\n",
    "                    input_batch = input_batch.to('cuda')\n",
    "                    model.to('cuda')\n",
    "            else:\n",
    "                input_batch = input_batch.to('cpu')\n",
    "                model.to('cpu')\n",
    "              \n",
    "            with torch.no_grad():\n",
    "                output = model(input_batch)\n",
    "\n",
    "            # Tensor of shape 1000, with confidence scores over Imagenet's 1000 classes\n",
    "            #print(output[0])\n",
    "\n",
    "            # The output has unnormalized scores. To get probabilities, you can run a softmax on it.\n",
    "            probabilities = torch.nn.functional.softmax(output[0], dim=0)\n",
    "            #print(probabilities)\n",
    "\n",
    "            # Show top categories per image\n",
    "            top5_prob, top5_catid = torch.topk(probabilities, 5)\n",
    "            for i in range(top5_prob.size(0)):\n",
    "                #print(top5_catid[i])\n",
    "                #print(imgidx, ' ', categories[top5_catid[i]], top5_prob[i].item())\n",
    "\n",
    "                if( top5_catid[i] == idx_gt[imgidx] ):\n",
    "                    top1_cnt += 1\n",
    "\n",
    "            imgidx += 1\n",
    "\n",
    "        end = time.time() # end timer\n",
    "        print('n = ', n)\n",
    "        print('top1_cnt = ', top1_cnt)\n",
    "        print('top1_cnt/n = ', top1_cnt/n)\n",
    "        print('time = ', end - start)\n",
    "        print('')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
